#!/usr/bin/env python3
"""Generate a versioned baseline snapshot from smoke metrics.

Reads the current smoke metrics (schema v1) and produces:
  1. eval/snapshots/baseline.json  — machine-readable baseline with provenance
  2. eval/reports/baseline-report.md — human-readable summary for all lanes

Exit codes:
  0 - Baseline published successfully
  2 - Error (missing files, git issues, etc.)
"""
from __future__ import annotations

import argparse
import json
import subprocess
import sys
from datetime import datetime, timezone
from hashlib import sha256
from pathlib import Path


def _git(*args: str) -> str:
    result = subprocess.run(
        ["git", *args],
        capture_output=True,
        text=True,
        check=True,
    )
    return result.stdout.strip()


def _lockfile_sha256(lockfile: Path) -> str:
    return sha256(lockfile.read_bytes()).hexdigest()


def _build_provenance(lockfile: Path, seed: int) -> dict:
    commit_sha = _git("rev-parse", "HEAD")
    commit_short = _git("rev-parse", "--short=10", "HEAD")
    commit_date = _git("log", "-1", "--format=%aI")
    branch = _git("branch", "--show-current") or _git("rev-parse", "--abbrev-ref", "HEAD")

    return {
        "commit_sha": commit_sha,
        "commit_short": commit_short,
        "commit_date": commit_date,
        "branch": branch,
        "datasets_lock_sha256": _lockfile_sha256(lockfile),
        "eval_seed": seed,
        "run_timestamp": datetime.now(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ"),
    }


def _build_baseline(metrics: dict, provenance: dict) -> dict:
    raw = metrics["metrics"]

    baseline: dict = {
        "schema_version": 2,
        "description": "Stock Ghidra baseline metrics for semantic search and type recovery",
        "provenance": provenance,
        "thresholds": {
            "regression_tolerance": 0.05,
            "description": "Max acceptable metric drop (absolute) before flagging regression",
        },
        "metrics": {},
    }

    # Similarity
    sim = raw.get("similarity", {})
    baseline["metrics"]["similarity"] = {
        "description": "Semantic search baseline (stock Ghidra heuristic)",
        "dataset": "toy-similarity-v1",
        "method": "jaccard_token",
        "recall@1": {
            "value": sim.get("recall@1", 0.0),
            "min_threshold": max(sim.get("recall@1", 0.0) - 0.05, 0.0),
            "target": 0.70,
            "note": "Target from REFuSe-Bench cross-compiler evaluation",
        },
        "mrr": {
            "value": sim.get("mrr", 0.0),
            "min_threshold": max(sim.get("mrr", 0.0) - 0.05, 0.0),
            "target": 0.75,
            "note": "Mean reciprocal rank target",
        },
        "queries": sim.get("queries", 0),
    }

    # Type recovery
    typ = raw.get("type", {})
    baseline["metrics"]["type"] = {
        "description": "Type recovery baseline (stock Ghidra heuristic)",
        "dataset": "toy-type-v1",
        "method": "heuristic_name_pattern",
        "accuracy": {
            "value": typ.get("accuracy", 0.0),
            "min_threshold": max(typ.get("accuracy", 0.0) - 0.05, 0.0),
            "target": 0.25,
            "note": "Target: 60% improvement over ~15% Ghidra baseline (SURE 2025)",
        },
        "cases": typ.get("cases", 0),
    }

    # Diffing
    diff = raw.get("diff", {})
    baseline["metrics"]["diff"] = {
        "description": "Binary diffing baseline (stock Ghidra heuristic)",
        "dataset": "toy-diff-v1",
        "method": "name_match",
        "match_rate": {
            "value": diff.get("match_rate", 0.0),
            "min_threshold": max(diff.get("match_rate", 0.0) - 0.05, 0.0),
            "target": 0.90,
            "note": "Target: function match rate on PatchCorpus",
        },
        "coverage": {
            "value": diff.get("coverage", 0.0),
            "min_threshold": max(diff.get("coverage", 0.0) - 0.05, 0.0),
        },
        "pairs": diff.get("pairs", 0),
    }

    return baseline


def _format_report(baseline: dict) -> str:
    prov = baseline["provenance"]
    metrics = baseline["metrics"]

    lines = [
        "# Baseline Report: Stock Ghidra Metrics",
        "",
        "> Auto-generated by `eval/scripts/publish_baseline.py`.",
        "> Do not edit manually; re-run the publish script to update.",
        "",
        "## Provenance",
        "",
        f"| Field | Value |",
        f"|---|---|",
        f"| Commit | `{prov['commit_short']}` (`{prov['commit_sha']}`) |",
        f"| Date | {prov['commit_date']} |",
        f"| Branch | `{prov['branch']}` |",
        f"| Datasets lock SHA-256 | `{prov['datasets_lock_sha256'][:16]}...` |",
        f"| Eval seed | {prov['eval_seed']} |",
        f"| Run timestamp | {prov['run_timestamp']} |",
        "",
        "## Benchmark Slices",
        "",
        "| Slice | Dataset | Method | Size |",
        "|---|---|---|---|",
    ]

    for area, data in sorted(metrics.items()):
        dataset = data.get("dataset", "unknown")
        method = data.get("method", "unknown")
        size_key = next(
            (k for k in ("queries", "cases", "pairs") if k in data), None
        )
        size = data.get(size_key, "?") if size_key else "?"
        lines.append(
            f"| {area.replace('_', ' ').title()} | `{dataset}` | {method} | {size} |"
        )

    lines += [
        "",
        "## Results",
        "",
        "| Area | Metric | Baseline Value | Min Threshold | Target | Status |",
        "|---|---|---|---|---|---|",
    ]

    for area, data in sorted(metrics.items()):
        for key, info in sorted(data.items()):
            if not isinstance(info, dict) or "value" not in info:
                continue
            val = info["value"]
            thr = info.get("min_threshold", "N/A")
            target = info.get("target", "N/A")
            thr_str = f"{thr:.4f}" if isinstance(thr, (int, float)) else str(thr)
            target_str = f"{target:.4f}" if isinstance(target, (int, float)) else str(target)
            lines.append(
                f"| {area.replace('_', ' ').title()} | {key} | {val:.4f} | {thr_str} | {target_str} | Measured |"
            )

    lines += [
        "",
        "## Interpretation",
        "",
        "These baselines represent **stock Ghidra** performance on toy benchmark slices.",
        "They serve as the comparator for all subsequent improvements.",
        "",
        "- **Semantic search**: Jaccard token similarity over function descriptions.",
        "  The toy corpus uses natural-language descriptions, so high recall is expected.",
        "  Real benchmarks (REFuSe-Bench, BinaryCorp) will use binary features.",
        "- **Type recovery**: Heuristic name-pattern matching.",
        "  The toy cases test obvious patterns (`len` -> `size_t`). Real benchmarks (SURE 2025)",
        "  measure against DWARF ground truth at -O0/-O2 optimization levels.",
        "- **Diffing**: Name-based function matching between two binaries.",
        "  Real benchmarks (PatchCorpus) include renamed/refactored functions.",
        "",
        "## Regression Policy",
        "",
        f"- **Tolerance**: {baseline['thresholds']['regression_tolerance']:.2f} (absolute drop allowed)",
        "- Per-commit smoke checks compare against this baseline via `eval/scripts/check_regression.py`.",
        "- Any metric dropping below its `min_threshold` blocks the build.",
        "",
        "## Next Steps",
        "",
        "1. Scale to real benchmark datasets (REFuSe-Bench, SURE 2025, PatchCorpus).",
        "2. Add per-commit smoke checks to CI.",
        "3. Establish nightly regression runs with `eval/scripts/compare_runs.py`.",
        "4. Track metric trends in the evaluation dashboard.",
        "",
        "## References",
        "",
        "- Evaluation harness spec: `docs/research/evaluation-harness.md`",
        "- Execution board: `docs/execution-board-12-weeks.md`",
        "- Dataset lockfile: `datasets/datasets.lock.json`",
        "- Smoke runner: `eval/run_smoke.sh`",
        "",
    ]

    return "\n".join(lines)


def main(argv: list[str] | None = None) -> int:
    parser = argparse.ArgumentParser(description="Publish versioned baseline snapshot and report")
    parser.add_argument(
        "--metrics",
        type=Path,
        default=Path("eval/output/smoke/metrics.json"),
        help="Path to smoke metrics JSON",
    )
    parser.add_argument(
        "--lockfile",
        type=Path,
        default=Path("datasets/datasets.lock.json"),
        help="Path to datasets lockfile",
    )
    parser.add_argument(
        "--baseline-out",
        type=Path,
        default=Path("eval/snapshots/baseline.json"),
        help="Path to write baseline JSON",
    )
    parser.add_argument(
        "--report-out",
        type=Path,
        default=Path("eval/reports/baseline-report.md"),
        help="Path to write baseline report Markdown",
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=0,
        help="Eval seed used for the smoke run",
    )
    args = parser.parse_args(argv)

    if not args.metrics.exists():
        print(f"[publish] ERROR: metrics file not found: {args.metrics}", file=sys.stderr)
        print("[publish] Hint: run `bash eval/run_smoke.sh` first.", file=sys.stderr)
        return 2

    try:
        metrics = json.loads(args.metrics.read_text(encoding="utf-8"))
    except Exception as exc:
        print(f"[publish] ERROR: invalid metrics JSON: {exc}", file=sys.stderr)
        return 2

    if metrics.get("schema_version") != 1:
        print("[publish] ERROR: unsupported metrics schema_version", file=sys.stderr)
        return 2

    try:
        provenance = _build_provenance(args.lockfile, args.seed)
    except subprocess.CalledProcessError as exc:
        print(f"[publish] ERROR: git command failed: {exc}", file=sys.stderr)
        return 2

    baseline = _build_baseline(metrics, provenance)

    # Write baseline JSON
    args.baseline_out.parent.mkdir(parents=True, exist_ok=True)
    args.baseline_out.write_text(
        json.dumps(baseline, indent=2) + "\n", encoding="utf-8"
    )
    print(f"[publish] wrote {args.baseline_out}")

    # Write report
    report = _format_report(baseline)
    args.report_out.parent.mkdir(parents=True, exist_ok=True)
    args.report_out.write_text(report, encoding="utf-8")
    print(f"[publish] wrote {args.report_out}")

    return 0


if __name__ == "__main__":
    raise SystemExit(main())
