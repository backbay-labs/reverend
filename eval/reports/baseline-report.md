# Baseline Report: Stock Ghidra Metrics

> Auto-generated by `eval/scripts/publish_baseline.py`.
> Do not edit manually; re-run the publish script to update.

## Provenance

| Field | Value |
|---|---|
| Commit | `9cd9bbb1de` (`9cd9bbb1dee22469fc8b157750b9831511f40f56`) |
| Date | 2026-02-19T22:29:50-07:00 |
| Branch | `wc/1003/20260220T054303Z` |
| Datasets lock SHA-256 | `bdb02a18c04a144d...` |
| Eval seed | 0 |
| Run timestamp | 2026-02-20T05:47:44Z |

## Benchmark Slices

| Slice | Dataset | Method | Size |
|---|---|---|---|
| Diff | `toy-diff-v1` | name_match | 3 |
| Similarity | `toy-similarity-v1` | jaccard_token | 3 |
| Type | `toy-type-v1` | heuristic_name_pattern | 5 |

## Results

| Area | Metric | Baseline Value | Min Threshold | Target | Status |
|---|---|---|---|---|---|
| Diff | coverage | 0.6667 | 0.6167 | N/A | Measured |
| Diff | match_rate | 0.6667 | 0.6167 | 0.9000 | Measured |
| Similarity | mrr | 1.0000 | 0.9500 | 0.7500 | Measured |
| Similarity | recall@1 | 1.0000 | 0.9500 | 0.7000 | Measured |
| Type | accuracy | 1.0000 | 0.9500 | 0.2500 | Measured |

## Interpretation

These baselines represent **stock Ghidra** performance on toy benchmark slices.
They serve as the comparator for all subsequent improvements.

- **Semantic search**: Jaccard token similarity over function descriptions.
  The toy corpus uses natural-language descriptions, so high recall is expected.
  Real benchmarks (REFuSe-Bench, BinaryCorp) will use binary features.
- **Type recovery**: Heuristic name-pattern matching.
  The toy cases test obvious patterns (`len` -> `size_t`). Real benchmarks (SURE 2025)
  measure against DWARF ground truth at -O0/-O2 optimization levels.
- **Diffing**: Name-based function matching between two binaries.
  Real benchmarks (PatchCorpus) include renamed/refactored functions.

## Regression Policy

- **Tolerance**: 0.05 (absolute drop allowed)
- Per-commit smoke checks compare against this baseline via `eval/scripts/check_regression.py`.
- Any metric dropping below its `min_threshold` blocks the build.

## Next Steps

1. Scale to real benchmark datasets (REFuSe-Bench, SURE 2025, PatchCorpus).
2. Add per-commit smoke checks to CI.
3. Establish nightly regression runs with `eval/scripts/compare_runs.py`.
4. Track metric trends in the evaluation dashboard.

## References

- Evaluation harness spec: `docs/research/evaluation-harness.md`
- Execution board: `docs/execution-board-12-weeks.md`
- Dataset lockfile: `datasets/datasets.lock.json`
- Smoke runner: `eval/run_smoke.sh`
